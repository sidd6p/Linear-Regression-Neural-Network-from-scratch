{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/siddp6/linear-regression-neural-network-from-scratch?scriptVersionId=175106186\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"546a1ad5","metadata":{"papermill":{"duration":0.009268,"end_time":"2024-05-01T16:09:55.849108","exception":false,"start_time":"2024-05-01T16:09:55.83984","status":"completed"},"tags":[]},"source":["## Intutions\n","We have an equation like this: **y = ax₁ + bx₂ + cx₃ + dx₄ + ex₅ + bias**. This is a very familiar equation. We have 5 input variables called **x₁, x₂, x₃, x₄, and x₅**. We also have 5 coefficient constants called **a, b, c, d, e** and one **bias**. And one output variable called **y**. This is called a linear equation.\n","\n","Here is what we are going to do:\n","- Create 5000 random input sets, each set will have 5 inputs **(x₁, x₂, x₃, x₄, x₅)**.\n","- Create 5 coefficients.\n","- Create one bias.\n","- Calculate 5000 outputs based on the above equation.\n","\n","Our Goal is to create a Linear Model that can give predict the correct output **(y)** based on input **(x₁, x₂, x₃, x₄, x₅)**. And the catch is that, our model does not know the coefficients and bias.\n"]},{"cell_type":"markdown","id":"bbb2ccaf","metadata":{"papermill":{"duration":0.008416,"end_time":"2024-05-01T16:09:55.866599","exception":false,"start_time":"2024-05-01T16:09:55.858183","status":"completed"},"tags":[]},"source":["## Linear Model\n","- We will initialize 5 random values to consider as coefficients called as weights and one more random value to consider as bias called as bias. \n","- We will pass each input set to the model, it will generate a value based on input, weights, and bias using the above equation.\n","- We will calculate the loss by comparing the predicted value and actual value (y). We use **loss function** for this.\n","- Based on the loss, the model will change the weights and bias. It uses **gradient** to determine if it should increase a particular weight or bias or decrease. And also, we give another input to the model called **learning rate** to determine the magnitude of change.\n","- Our end goal is that the model's weights and bias should be near to the coefficients and original bias. More close the weights and bias will be to the coefficients and original bias, more correctly our model can correct y based on input.\n"]},{"cell_type":"markdown","id":"f479ef3e","metadata":{"papermill":{"duration":0.008388,"end_time":"2024-05-01T16:09:55.884123","exception":false,"start_time":"2024-05-01T16:09:55.875735","status":"completed"},"tags":[]},"source":["### Gradients\n","\n","- **Definition**: Gradients represent the rate of change of a function with respect to its parameters. In the context of neural networks, these parameters are the weights and biases of the network.\n","\n","- **Purpose**: Gradients are crucial for optimizing the parameters of a neural network during the training process. By computing gradients with respect to a loss function, we can determine how each parameter should be adjusted to minimize the loss and improve the network's performance.\n","\n","- **Backpropagation**: Gradients are computed using the backpropagation algorithm, which efficiently calculates the gradients of the loss function with respect to each parameter in the network by propagating error gradients backward through the network.\n","\n","*Imagine you're trying to bake the perfect cake, but you're not quite sure how much of each ingredient to use. Gradients would be like telling you how the taste of the cake changes when you slightly adjust the amount of sugar, flour, or eggs. With this information, you can gradually tweak the recipe to make the cake taste just right.*\n","\n","*In machine learning, gradients tell us how much we need to adjust the parameters (like weights in a neural network) to make our model perform better at its task. By following the gradients, the model can learn from its mistakes and improve over time, just like a chef perfecting a recipe with each attempt.*"]},{"cell_type":"markdown","id":"54c48768","metadata":{"papermill":{"duration":0.008375,"end_time":"2024-05-01T16:09:55.901149","exception":false,"start_time":"2024-05-01T16:09:55.892774","status":"completed"},"tags":[]},"source":["### Loss function\n","\n","\n","A loss function measures how well a machine learning model performs by comparing its predictions to the actual target values. It quantifies the error between predicted and actual values. The goal is to minimize this error during training"]},{"cell_type":"markdown","id":"7da29d56","metadata":{"papermill":{"duration":0.008484,"end_time":"2024-05-01T16:09:55.918204","exception":false,"start_time":"2024-05-01T16:09:55.90972","status":"completed"},"tags":[]},"source":["### Imports"]},{"cell_type":"code","execution_count":1,"id":"c8b02202","metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:09:55.939074Z","iopub.status.busy":"2024-05-01T16:09:55.938173Z","iopub.status.idle":"2024-05-01T16:10:00.327374Z","shell.execute_reply":"2024-05-01T16:10:00.325851Z"},"papermill":{"duration":4.4049,"end_time":"2024-05-01T16:10:00.332319","exception":false,"start_time":"2024-05-01T16:09:55.927419","status":"completed"},"tags":[]},"outputs":[],"source":["import numpy as np\n","import torch"]},{"cell_type":"markdown","id":"0ce95315","metadata":{"papermill":{"duration":0.008253,"end_time":"2024-05-01T16:10:00.349404","exception":false,"start_time":"2024-05-01T16:10:00.341151","status":"completed"},"tags":[]},"source":["### Data"]},{"cell_type":"markdown","id":"0bf88d69","metadata":{"papermill":{"duration":0.008425,"end_time":"2024-05-01T16:10:00.366332","exception":false,"start_time":"2024-05-01T16:10:00.357907","status":"completed"},"tags":[]},"source":["**inputs_np = np.random.randint(10, 100, size=(num_samples, num_features)).astype('float32)**\n","\n","- `np.random.randint(10, 100, size=(num_samples, num_features))`: This function call generates random integers between 10 and 100 (inclusive) with a specified shape `(num_samples, num_features)`. `\n","\n","-  num_samples` represents the number of samples or rows, and `num_features` represents the number of features or columns in the array.\n","\n","- `.astype('float32')`: This part of the code converts the generated random integers to floating-point numbers of 32-bit precision (`float32`). \n","\n","So, in summary, `inputs_np` will be a NumPy array filled with random floating-point numbers, with `num_samples` rows and `num_features` columns, where each element falls between 10 and 100.\n"]},{"cell_type":"markdown","id":"3f595916","metadata":{"papermill":{"duration":0.008272,"end_time":"2024-05-01T16:10:00.383093","exception":false,"start_time":"2024-05-01T16:10:00.374821","status":"completed"},"tags":[]},"source":["**inputs_tensor = torch.tensor(inputs_np, requires_grad=False)**\n","- `torch.tensor(inputs_np, requires_grad=False)`: This function call converts the NumPy array `inputs_np` into a PyTorch tensor. The `requires_grad=False` argument specifies that gradients should not be computed for this tensor. Gradients are crucial for training neural networks using techniques like backpropagation, but in this case, they are not needed, which can save memory and computation resources.\n","\n","So, in summary, `inputs_tensor` will be a PyTorch tensor containing the same data as `inputs_np`, without tracking gradients.\n"]},{"cell_type":"markdown","id":"8ec69c9d","metadata":{"papermill":{"duration":0.008368,"end_time":"2024-05-01T16:10:00.402083","exception":false,"start_time":"2024-05-01T16:10:00.393715","status":"completed"},"tags":[]},"source":["**bias_tensor = torch.tensor([4], dtype=torch.float32, requires_grad=False)**\n"," \n","- `torch.tensor([4])`: This creates a PyTorch tensor with a single element `[4]`. The tensor contains one element, which is the number 4. This is a one-dimensional tensor because it has only one axis.\n","  \n","- `dtype=torch.float32`: This specifies the data type of the tensor. In this case, it's set to `torch.float32`, which means the tensor elements are 32-bit floating-point numbers.\n","\n","- `requires_grad=False`: This parameter indicates whether PyTorch should track operations on this tensor to automatically compute gradients later for optimization algorithms like gradient descent. Setting it to `False` means that gradients won't be calculated for this tensor. \n","\n","So, overall, this line of code creates a simple one-dimensional tensor containing the number 4, with data type set to 32-bit floating point, and it won't require gradients for any operations performed on it.\n","\n"]},{"cell_type":"markdown","id":"a09240a7","metadata":{"papermill":{"duration":0.009695,"end_time":"2024-05-01T16:10:00.421996","exception":false,"start_time":"2024-05-01T16:10:00.412301","status":"completed"},"tags":[]},"source":["**targets_tensor = torch.matmul(inputs_tensor, coefficients_tensor) + bias_tensor**\n","- `torch.matmul(inputs_tensor, coefficients_tensor)`: This computes the matrix multiplication of `inputs_tensor` and `coefficients_tensor`. \n","\n","- `+ bias_tensor`: This adds the `bias_tensor` to the result of the matrix multiplication. \n","\n","So, overall, this line of code computes the linear transformation of `inputs_tensor` using `coefficients_tensor`, adds the bias represented by `bias_tensor`, and stores the result in `targets_tensor`."]},{"cell_type":"code","execution_count":2,"id":"6e10db60","metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:10:00.44335Z","iopub.status.busy":"2024-05-01T16:10:00.442233Z","iopub.status.idle":"2024-05-01T16:10:00.533392Z","shell.execute_reply":"2024-05-01T16:10:00.531972Z"},"papermill":{"duration":0.10582,"end_time":"2024-05-01T16:10:00.537174","exception":false,"start_time":"2024-05-01T16:10:00.431354","status":"completed"},"tags":[]},"outputs":[],"source":["num_samples = 5000\n","num_features = 5\n","\n","inputs_np = np.random.randint(10, 100, size=(num_samples, num_features)).astype('float32')\n","inputs_tensor = torch.tensor(inputs_np, requires_grad=False) \n","\n","coefficients_tensor = torch.tensor([2, 8, 1, 3, 6], dtype=torch.float32, requires_grad=False)\n","bias_tensor = torch.tensor([4], dtype=torch.float32, requires_grad=False)\n","\n","targets_tensor = torch.matmul(inputs_tensor, coefficients_tensor) + bias_tensor"]},{"cell_type":"code","execution_count":3,"id":"9b745034","metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:10:00.557146Z","iopub.status.busy":"2024-05-01T16:10:00.556598Z","iopub.status.idle":"2024-05-01T16:10:00.6266Z","shell.execute_reply":"2024-05-01T16:10:00.625259Z"},"papermill":{"duration":0.083075,"end_time":"2024-05-01T16:10:00.629495","exception":false,"start_time":"2024-05-01T16:10:00.54642","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Input features for the 3rd sample: [97. 50. 94. 13. 95.]\n","Coefficients: tensor([2., 8., 1., 3., 6.])\n","Bias: tensor([4.])\n","Target value for the 3rd sample computed using matrix multiplication and bias addition: tensor(1301.)\n"]}],"source":["print(\"Input features for the 3rd sample:\", inputs_np[2])\n","print(\"Coefficients:\", coefficients_tensor)\n","print(\"Bias:\", bias_tensor)\n","print(\"Target value for the 3rd sample computed using matrix multiplication and bias addition:\", targets_tensor[2])"]},{"cell_type":"markdown","id":"40c62e70","metadata":{"papermill":{"duration":0.008675,"end_time":"2024-05-01T16:10:00.647286","exception":false,"start_time":"2024-05-01T16:10:00.638611","status":"completed"},"tags":[]},"source":["## Model"]},{"cell_type":"markdown","id":"ab325fb6","metadata":{"papermill":{"duration":0.018281,"end_time":"2024-05-01T16:10:00.67426","exception":false,"start_time":"2024-05-01T16:10:00.655979","status":"completed"},"tags":[]},"source":["**w = torch.tensor([1, 1, 1, 1, 1], dtype=torch.float32, requires_grad=True)**\n","\n","- `torch.tensor`: This is a function from the PyTorch library used to create a tensor. A tensor is a multi-dimensional array similar to NumPy arrays but with additional functionalities tailored for deep learning.\n","\n","- `[1, 1, 1, 1, 1]`: This list contains the values that will be stored in the tensor. In this case, it's a one-dimensional tensor with five elements, all initialized to the value 1.\n","\n","- `dtype=torch.float32`: This specifies the data type of the tensor. `torch.float32` indicates that the tensor will store floating-point numbers (32-bit floating point precision).\n","\n","- `requires_grad=True`: This parameter indicates that operations involving this tensor should be tracked for gradient computation during backpropagation. This is essential for automatic differentiation, a key component of training neural networks using techniques like gradient descent.\n","\n","So, in summary, this line of code creates a PyTorch tensor `w` with five elements, all initialized to 1, with a data type of 32-bit floating-point, and specifies that gradients should be computed with respect to this tensor during backpropagation."]},{"cell_type":"markdown","id":"72b85eed","metadata":{"papermill":{"duration":0.009124,"end_time":"2024-05-01T16:10:00.692713","exception":false,"start_time":"2024-05-01T16:10:00.683589","status":"completed"},"tags":[]},"source":["**b = torch.tensor([1], dtype=torch.float32, requires_grad=True)**\n","\n","- `torch.tensor`: This function from the PyTorch library is used to create a tensor.\n","\n","- `[1]`: This list contains the value that will be stored in the tensor. In this case, it's a one-dimensional tensor with a single element, initialized to the value 1.\n","\n","- `dtype=torch.float32`: This specifies the data type of the tensor. `torch.float32` indicates that the tensor will store floating-point numbers (32-bit floating point precision).\n","\n","- `requires_grad=True`: This parameter indicates that operations involving this tensor should be tracked for gradient computation during backpropagation. Similar to the previous example, this enables automatic differentiation for this tensor.\n","\n","In summary, this line of code creates a PyTorch tensor `b` with a single element initialized to 1, with a data type of 32-bit floating-point, and specifies that gradients should be computed with respect to this tensor during backpropagation."]},{"cell_type":"code","execution_count":4,"id":"e1c29dcc","metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:10:00.71529Z","iopub.status.busy":"2024-05-01T16:10:00.714441Z","iopub.status.idle":"2024-05-01T16:10:00.721919Z","shell.execute_reply":"2024-05-01T16:10:00.720819Z"},"papermill":{"duration":0.021632,"end_time":"2024-05-01T16:10:00.724649","exception":false,"start_time":"2024-05-01T16:10:00.703017","status":"completed"},"tags":[]},"outputs":[],"source":["w = torch.tensor([1, 1, 1, 1, 1], dtype=torch.float32, requires_grad=True)\n","b = torch.tensor([1], dtype=torch.float32, requires_grad=True)"]},{"cell_type":"code","execution_count":5,"id":"177d7da1","metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:10:00.747476Z","iopub.status.busy":"2024-05-01T16:10:00.746655Z","iopub.status.idle":"2024-05-01T16:10:00.756948Z","shell.execute_reply":"2024-05-01T16:10:00.755161Z"},"papermill":{"duration":0.025926,"end_time":"2024-05-01T16:10:00.760574","exception":false,"start_time":"2024-05-01T16:10:00.734648","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Model weight tensor([1., 1., 1., 1., 1.], requires_grad=True) and bias tensor([1.], requires_grad=True)\n","Actual coefficients tensor([2., 8., 1., 3., 6.]) and bias tensor([4.])\n","Our goal is to train the model, that it adjust the weight and its bias to the actual coefficients and bias\n"]}],"source":["print(f\"Model weight {w} and bias {b}\")\n","print(f\"Actual coefficients {coefficients_tensor} and bias {bias_tensor}\")\n","print(\"Our goal is to train the model, that it adjust the weight and its bias to the actual coefficients and bias\")"]},{"cell_type":"markdown","id":"bbfcac18","metadata":{"papermill":{"duration":0.009165,"end_time":"2024-05-01T16:10:00.779283","exception":false,"start_time":"2024-05-01T16:10:00.770118","status":"completed"},"tags":[]},"source":["**torch.sum(diff*diff) / diff.numel()**\n"," - `torch.sum(diff*diff)`: This computes the sum of the squared differences between corresponding elements of `t1` and `t2`. Squaring the differences ensures that negative differences do not cancel out positive differences, emphasizing larger errors.\n","   \n"," - `diff.numel()`: This function calculates the total number of elements in the tensor `diff`. It's used to normalize the sum of squared differences by the total number of elements, yielding the mean squared error."]},{"cell_type":"code","execution_count":6,"id":"0b43bc0a","metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:10:00.806623Z","iopub.status.busy":"2024-05-01T16:10:00.806208Z","iopub.status.idle":"2024-05-01T16:10:00.813369Z","shell.execute_reply":"2024-05-01T16:10:00.811714Z"},"papermill":{"duration":0.022877,"end_time":"2024-05-01T16:10:00.816819","exception":false,"start_time":"2024-05-01T16:10:00.793942","status":"completed"},"tags":[]},"outputs":[],"source":["\"\"\"\n","This is a loss function\n","This function returns the average squared difference between t1 and t2, which is a common metric used as a loss function in regression problems. \n","The aim during training is to minimize this value, meaning the model's predictions are as close to the actual targets as possible.\n","\"\"\"\n","def mse(t1, t2):\n","    diff = t1 - t2\n","    return torch.sum(diff*diff) / diff.numel()\n"]},{"cell_type":"markdown","id":"4fedc033","metadata":{"papermill":{"duration":0.0091,"end_time":"2024-05-01T16:10:00.836795","exception":false,"start_time":"2024-05-01T16:10:00.827695","status":"completed"},"tags":[]},"source":["**return x @ w.t() + b**\n","- `x`: This is the input data, typically a matrix where each row represents a sample and each column represents a feature.\n","- `@`: This symbol denotes matrix multiplication in Python.\n","- `w`: This is a tensor representing the weights of the model. It's typically a row vector where each element corresponds to the weight associated with each feature.\n","- `.t()`: This function transposes the tensor `w`, flipping its rows and columns. This is necessary to match the dimensions for matrix multiplication with `x`.\n","- `b`: This is a scalar representing the bias term of the model.\n","\n","So, `x @ w.t()` calculates the matrix multiplication of the input data `x` with the transposed weights `w`, resulting in a matrix where each row contains the predicted output for each sample.\n","\n","Finally, `+ b` adds the bias term to each row of the output matrix.\n"]},{"cell_type":"code","execution_count":7,"id":"cc6ab454","metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:10:00.858758Z","iopub.status.busy":"2024-05-01T16:10:00.858168Z","iopub.status.idle":"2024-05-01T16:10:00.865834Z","shell.execute_reply":"2024-05-01T16:10:00.864483Z"},"papermill":{"duration":0.021738,"end_time":"2024-05-01T16:10:00.868471","exception":false,"start_time":"2024-05-01T16:10:00.846733","status":"completed"},"tags":[]},"outputs":[],"source":["\"\"\"\n","A model is simple a function that return the output from the given input. \n","This is the linear model can be treat as mathematical function like y = ax + b, where a is the input cofficient, x is the input and b is the bias\n","\"\"\"\n","def model(x):\n","    return x @ w.t() + b"]},{"cell_type":"code","execution_count":8,"id":"0bb22dfb","metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:10:00.890415Z","iopub.status.busy":"2024-05-01T16:10:00.889214Z","iopub.status.idle":"2024-05-01T16:10:00.907703Z","shell.execute_reply":"2024-05-01T16:10:00.906058Z"},"papermill":{"duration":0.033101,"end_time":"2024-05-01T16:10:00.910473","exception":false,"start_time":"2024-05-01T16:10:00.877372","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([118.]) tensor([21.], grad_fn=<AddBackward0>)\n"]}],"source":["test_input = torch.tensor([2, 8, 1, 3, 6], dtype=torch.float32, requires_grad=False)\n","actual_output = test_input[0] * coefficients_tensor[0] +\\\n","                test_input[1] * coefficients_tensor[1] +\\\n","                test_input[2] * coefficients_tensor[2] +\\\n","                test_input[3] * coefficients_tensor[3] +\\\n","                test_input[4] * coefficients_tensor[4] +\\\n","                bias_tensor\n","\n","model_ouput = model(test_input)\n","\n","print(actual_output, model_ouput) # They are not equal at all"]},{"cell_type":"markdown","id":"e92a597f","metadata":{"papermill":{"duration":0.008989,"end_time":"2024-05-01T16:10:00.93093","exception":false,"start_time":"2024-05-01T16:10:00.921941","status":"completed"},"tags":[]},"source":["- `with torch.no_grad():`: This line enters a context where gradients are not tracked. This is done to perform parameter updates without affecting the gradient computation graph.\n","\n","- `w -= w.grad * lr`: This line updates the weights (`w`) using gradient descent. It subtracts the product of the gradients (`w.grad`) and the learning rate (`lr`) from the current weights.\n","\n","- `b -= b.grad * lr`: This line updates the bias (`b`) in a similar manner as the weights.\n","\n","- `w.grad.zero_()`: This line resets the gradients of the weights to zero. This is necessary because PyTorch accumulates gradients by default, and we want to clear them before the next iteration.\n","\n","- `b.grad.zero_()`: This line resets the gradients of the bias to zero, similar to what was done for the weights.\n","\n","These lines collectively represent one iteration (or one epoch) of the training loop, where predictions are made, the loss is calculated, gradients are computed, and parameters are updated using gradient descent."]},{"cell_type":"code","execution_count":9,"id":"574c4037","metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:10:00.953363Z","iopub.status.busy":"2024-05-01T16:10:00.952183Z","iopub.status.idle":"2024-05-01T16:10:01.145827Z","shell.execute_reply":"2024-05-01T16:10:01.144436Z"},"papermill":{"duration":0.209077,"end_time":"2024-05-01T16:10:01.149607","exception":false,"start_time":"2024-05-01T16:10:00.94053","status":"completed"},"tags":[]},"outputs":[],"source":["\"\"\"\n","Epochs is the number of time we want to train the input on the same data, with each training model is expected to give better results\n","lr is the learning rate, that maginitute with which we want to chnage the weights and bias\n","So as we see earlier that model has weights and bias that are set to random value initally.\n","In each epoch, we chnage these values by lr in such way that we should match the weight and bias to the actual cofficient and bias.\n","\"\"\"\n","epochs = 500\n","lr = 1e-5\n","\n","for epoch in range(epochs):\n","    preds = model(inputs_tensor) # generate the prediction from model\n","    \n","    loss = mse(preds, targets_tensor) # calculate the loss\n","    loss.backward() # calulate the gradient\n","    \n","    # Adjust the weight and bias based on gradient and learning rate\n","    with torch.no_grad():\n","        w -= w.grad * lr\n","        b -= b.grad * lr\n","        \n","        w.grad.zero_()\n","        b.grad.zero_()"]},{"cell_type":"markdown","id":"908e2941","metadata":{"papermill":{"duration":0.009121,"end_time":"2024-05-01T16:10:01.171354","exception":false,"start_time":"2024-05-01T16:10:01.162233","status":"completed"},"tags":[]},"source":["## Results"]},{"cell_type":"code","execution_count":10,"id":"c6780ab2","metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:10:01.199927Z","iopub.status.busy":"2024-05-01T16:10:01.197956Z","iopub.status.idle":"2024-05-01T16:10:01.209102Z","shell.execute_reply":"2024-05-01T16:10:01.207993Z"},"papermill":{"duration":0.029003,"end_time":"2024-05-01T16:10:01.212613","exception":false,"start_time":"2024-05-01T16:10:01.18361","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Model weights tensor([2.0122, 8.0062, 1.0132, 3.0110, 6.0088], requires_grad=True) and bias tensor([1.0546], requires_grad=True)\n","Actual coefficients tensor([2., 8., 1., 3., 6.]) and bias tensor([4.])\n"]}],"source":["print(f\"Model weights {w} and bias {b}\")\n","print(f\"Actual coefficients {coefficients_tensor} and bias {bias_tensor}\")"]},{"cell_type":"markdown","id":"5e532b84","metadata":{"papermill":{"duration":0.008894,"end_time":"2024-05-01T16:10:01.232217","exception":false,"start_time":"2024-05-01T16:10:01.223323","status":"completed"},"tags":[]},"source":["> We can see that the model weights are almost equal to the coefficients. However, the model bias is not near to the actual bias. This can be achieved using some other advanced training techniques."]},{"cell_type":"code","execution_count":11,"id":"7539901b","metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:10:01.254982Z","iopub.status.busy":"2024-05-01T16:10:01.253825Z","iopub.status.idle":"2024-05-01T16:10:01.268126Z","shell.execute_reply":"2024-05-01T16:10:01.265588Z"},"papermill":{"duration":0.029652,"end_time":"2024-05-01T16:10:01.271024","exception":false,"start_time":"2024-05-01T16:10:01.241372","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([118.]) tensor([115.2278], grad_fn=<AddBackward0>)\n"]}],"source":["test_input = torch.tensor([2, 8, 1, 3, 6], dtype=torch.float32, requires_grad=False)\n","actual_output = test_input[0] * coefficients_tensor[0] +\\\n","                test_input[1] * coefficients_tensor[1] +\\\n","                test_input[2] * coefficients_tensor[2] +\\\n","                test_input[3] * coefficients_tensor[3] +\\\n","                test_input[4] * coefficients_tensor[4] +\\\n","                bias_tensor\n","\n","model_ouput = model(test_input)\n","\n","print(actual_output, model_ouput) # They both are almost equal now"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":10.293873,"end_time":"2024-05-01T16:10:02.308381","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-01T16:09:52.014508","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}